{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://nci.org.au/sites/default/files/logos/Logo-NCI.svg\" width=\"300\" align=\"left\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Processing MT data on the NCI\n",
    "\n",
    "### Requirements\n",
    "\n",
    "The following tutorial shows how to process magnetotelluric (MT) data stored on the National Computational Infrastructure (NCI). As such it makes certain assumptions about the storage structure, mainly that the time series data are stored in separate folders for each site and separate folders for each day. The tutorial uses the [Bounded Influence, Remote Reference Processing (BIRRP) code.](https://www.whoi.edu/science/AOPE/people/achave/Site/Next1.html)\n",
    "\n",
    "### Preparation\n",
    "\n",
    "For this example we will be processing the [Renmark 2009 time series data](http://dx.doi.org/10.25914/5bea5867bb322) located on NCI's /g/data file system. The first step in processing is to work out the starting and finishing dates for each site in the survey, in order to find the sites with overlapping time-series which can be used for remote-reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "def _linecount(filename):\n",
    "    \"\"\" Return number of lines in a file \"\"\"\n",
    "    num_lines = sum(1 for line in open(filename))\n",
    "    return num_lines\n",
    "\n",
    "def get_metadata(base_dir, frequency, tmp_dir):\n",
    "    \"\"\" Return information about each site \"\"\"\n",
    "    fn = os.path.join(tmp_dir, 'metadata.json')\n",
    "    if os.path.exists(fn):\n",
    "        with open(fn) as f:\n",
    "            sites = pickle.load(f)\n",
    "        return sites\n",
    "    sites = [os.path.basename(i) for i in glob(os.path.join(base_dir, '*'))]\n",
    "\n",
    "    sites = dict([(i, {}) for i in sites])\n",
    "\n",
    "    for site in sites.keys():\n",
    "        sites[site]['name'] = site\n",
    "        sites[site]['files'] = []\n",
    "        days = sorted([os.path.basename(i) for\n",
    "                       i in glob(os.path.join(base_dir, site, '*'))])\n",
    "        for idx, day in enumerate(days):\n",
    "            files = glob(os.path.join(base_dir, site, day, '*'))\n",
    "            if not files:\n",
    "                continue\n",
    "            last_files = files\n",
    "            sites[site]['files'].append(files)\n",
    "            if idx == 0:\n",
    "                start_time = os.path.basename(files[0]).split('_')[1].split('.')[0]\n",
    "                start_time = datetime.datetime.strptime(start_time,\n",
    "                                                        '%y%m%d%H%M%S')\n",
    "                sites[site]['start_time'] = start_time\n",
    "        if not [j for k in sites[site]['files'] for j in k]:\n",
    "            del sites[site]\n",
    "            continue\n",
    "        sites[site]['files'] = [j for k in sites[site]['files'] for j in k]\n",
    "        print(site, sites[site]['files'])\n",
    "        #if not files:\n",
    "        #    continue\n",
    "        files = last_files\n",
    "        end_date = os.path.basename(files[0]).split('_')[1].split('.')[0]\n",
    "        end_date = datetime.datetime.strptime(end_date, '%y%m%d%H%M%S')\n",
    "        length = _linecount(files[0])\n",
    "        end_time = end_date + datetime.timedelta(seconds=length/frequency)\n",
    "        sites[site]['end_time'] = end_time\n",
    "        sites[site]['samples'] = (sites[site]['end_time'] -\n",
    "                                  sites[site]['start_time']).seconds * frequency\n",
    "\n",
    "    with open(fn, 'w') as f:\n",
    "        pickle.dump(sites, f)\n",
    "    return sites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will read in the metadata required for processing a survey. After this processing it will write out this metadata into a file, which will be read back in the next time that the metadata is required.\n",
    "\n",
    "We will also make a function to calculate the overlap between two sites:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_intersection(local_site, remote_site):\n",
    "    \"\"\" Calculate time overlap of a local site and remote site \"\"\"\n",
    "    int_start = max(local_site['start_time'], remote_site['start_time'])\n",
    "    int_end = min(local_site['end_time'], remote_site['end_time'])\n",
    "    if (int_end - int_start).total_seconds()/60/60 < 5:\n",
    "        return\n",
    "    else:\n",
    "        return int_start, int_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the overlap is less than 5 hours then the function returns nothing, indicating no substantial overlap, otherwise it will return the start and end times of the intersection.\n",
    "\n",
    "The next stage is to make functions for writing out the overlap between the time series from two different sites as ASCII files for BIRRP use as inputs. Here one of the sites is designated as the remote reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "def decimate_file(fn, decimation, new_fn):\n",
    "    \"\"\" Read in a time series, apply a decimation and write out \"\"\"\n",
    "    print(fn, decimation, new_fn)\n",
    "    print(os.path.exists(new_fn))\n",
    "    if os.path.exists(new_fn):\n",
    "         return\n",
    "    print('decimating {}'.format(fn))\n",
    "    f = np.genfromtxt(fn)\n",
    "    f = scipy.signal.decimate(f, decimation, n=8)\n",
    "    np.savetxt(new_fn, f)\n",
    "\n",
    "\n",
    "def write_files(files, num_skip, num_samples, channel, out_dir, tstart, remote=False):\n",
    "    \"\"\" Write files \"\"\"\n",
    "    fn = 'local.' + channel if not remote else 'remote.' + channel\n",
    "    fn = tstart.strftime('%y%m%d%H%M%S') + '_' + fn\n",
    "    if os.path.exists(os.path.join(out_dir, fn)):\n",
    "        return\n",
    "    print('writing to {}'.format(os.path.join(out_dir, fn)))\n",
    "    ofile = open(os.path.join(out_dir, fn), 'w')\n",
    "    ifile = open(files.pop(0))\n",
    "    print(files, num_skip, num_samples)\n",
    "    for _ in range(num_skip):\n",
    "        try:\n",
    "            next(ifile)\n",
    "        except StopIteration:\n",
    "            ifile = open(files.pop(0))\n",
    "    for _ in range(num_samples):\n",
    "        try:\n",
    "            line = next(ifile)\n",
    "        except StopIteration:\n",
    "            try:\n",
    "                ifile = open(files.pop(0))\n",
    "                line = next(ifile)\n",
    "            except IndexError:\n",
    "                pass\n",
    "                # print('did not extract from {}'.format(files))\n",
    "        ofile.write(line)\n",
    "\n",
    "def write_birrp_inputs(local_site, remote_site, out_dir):\n",
    "    \"\"\" Write out the intersection of two files \"\"\"\n",
    "    if calc_intersection(local_site, remote_site):\n",
    "        int_start, int_end = calc_intersection(local_site, remote_site)\n",
    "    local_skip = (int_start - local_site['start_time']).total_seconds()*500\n",
    "    remote_skip = (int_start - remote_site['start_time']).total_seconds()*500\n",
    "    local_skip = int(local_skip)\n",
    "    remote_skip = int(remote_skip)\n",
    "    num_samples = int((int_end - int_start).total_seconds()*500)\n",
    "    if local_site['name'] == remote_site['name']:\n",
    "        remote_skip += 1\n",
    "        num_samples -= 1\n",
    "    dec_dir = os.path.join(out_dir, 'undecimated')\n",
    "    dec10_dir = os.path.join(out_dir, 'decimated10')\n",
    "    dec100_dir = os.path.join(out_dir, 'decimated100')\n",
    "    try:\n",
    "        os.makedirs(dec_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(dec10_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(dec100_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    channels = ['BX', 'BY', 'EX', 'EY']\n",
    "    for channel in channels:\n",
    "        files = sorted([i for i in local_site['files'] if channel in i])\n",
    "        write_files(files, local_skip, num_samples, channel, dec_dir, int_start)\n",
    "    for channel in [i for i in channels if 'B' in i]:\n",
    "        files = sorted([i for i in remote_site['files'] if channel in i])\n",
    "        write_files(files, remote_skip, num_samples, channel, dec_dir, int_start,\n",
    "                    remote=True)\n",
    "    for fn in glob(os.path.join(dec_dir, '*')):\n",
    "        if fn.endswith('X') or fn.endswith('Y'):\n",
    "            bn = os.path.basename(fn)\n",
    "            new_fn = os.path.join(dec10_dir, bn)\n",
    "            decimate_file(fn, 10, new_fn)\n",
    "            fn = new_fn\n",
    "            new_fn = os.path.join(dec100_dir, bn)\n",
    "            decimate_file(fn, 10, new_fn)\n",
    "    return num_samples, int_start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that decimations are also performed, with the decimated time series placed into new folders. By decimating the data we can achieve responses for longer periods than would otherwise be possible. Decimation is made using [scipy.signal.decimate](https://docs.scipy.org/doc/scipy-1.1.0/reference/generated/scipy.signal.decimate.html), which applies an anti-aliasing filter before decimating.\n",
    "\n",
    "For each three of these folders we generate a BIRRP script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_birrp_script(out_dir, samples, sample_rate, site_name, tstart):\n",
    "    birrp_string = '\\n'.join(['1', '2', '2', '2', '1', '3', '{2}',\n",
    "                              '65536,2,13', '3,1,3', 'y', '2',\n",
    "                              '0,0.0003,0.9999', '0.7', '0.0', '0.003,10000', '{3}', '0', '0',        \n",
    "                              '1', '10', '0', '0', '{1}', '0', '{4}_local.EY',\n",
    "                              '0', '0', '{4}_local.EX', '0', '0', '{4}_local.BY',\n",
    "                              '0', '0', '{4}_local.BX', '0', '0', '{4}_remote.BY',\n",
    "                              '0', '0', '{4}_remote.BX', '0', '0,90,0', '0,90,0',\n",
    "                              '0,90,0'])\n",
    "    birrp_string = birrp_string.format(os.path.join(out_dir, ''),\n",
    "                                       samples, sample_rate, site_name,\n",
    "                                       tstart.strftime('%y%m%d%H%M%S'))\n",
    "    return birrp_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BIRRP script has many different parameters and the best parameters will depend on the dataset. For more information see the [BIRRP manual](http://www.whoi.edu/science/AOPE/people/achave/Site/Next1_files/birrp.5.pdf).\n",
    "\n",
    "We also need a script to run BIRRP and to clean up the output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_birrp(out_dir, birrp_script, birrp_location):\n",
    "    script_fn = os.path.join(out_dir, 'script.txt')\n",
    "    with open(script_fn, 'w') as f:\n",
    "        f.write(birrp_script)\n",
    "    if any(fn.endswith('.j') for fn in os.listdir(out_dir)):\n",
    "        return\n",
    "    #os.system('mpirun -np 32 {} < {}'.format(birrp_location, script_fn)) ### Use if using the mpi modified version of BIRRP\n",
    "    os.system('{} < {}'.format(birrp_location, script_fn))\n",
    "    for f in glob('fft.*'):\n",
    "        os.remove(f)\n",
    "    for f in glob('remote.*') + glob('local.*'):  ### comment this for loop if you want to keep the intermediate files\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we would like a script to convert the outputs from our three frequency ranges into edi-files and to merge them into a single file. The merging should take place at the longest overlapping period, as this introduces the minimum amount of error resulting from aliasing during the decimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mtpy.core.edi\n",
    "from mtpy.uofa.qel_monitoring_j2edi import convert2edi\n",
    "from mtpy.uofa.simpleplotEDI import plotedi\n",
    "\n",
    "\n",
    "def process_birrp_results(out_dir, survey_cfg_fn, site_name, instr_resp_fn):\n",
    "    edi, coh = convert2edi(site_name, out_dir, survey_cfg_fn, instr_resp_fn, None)\n",
    "    metadata =  mtpy.utils.configfile.read_survey_configfile(survey_cfg_fn)[site_name.upper()]\n",
    "    e = mtpy.core.edi.Edi(edi)\n",
    "    if 'loc' in metadata:\n",
    "        e.head['loc'] = metadata['loc']\n",
    "    else:\n",
    "        e.head['loc'] = None\n",
    "    if 'acqdate' in metadata:\n",
    "        e.head['acqdate'] = metadata['acqdate']\n",
    "    else:\n",
    "        e.head['acqdate'] = None\n",
    "    if 'acquired_by' in metadata:\n",
    "        e.head['acqby'] = metadata['acquired_by']\n",
    "    else:\n",
    "        e.head['acqby'] = ''\n",
    "    edi = e.writefile(edi, allow_overwrite=True)\n",
    "    path = edi.split(\"qel\")[0]\n",
    "    edi_file = os.path.basename(edi)\n",
    "    new_name = edi_file.split(\"_\")[1]\n",
    "    new_edi_name = new_name + '.edi'\n",
    "    new_coh_name = new_name + '.coh'\n",
    "    new_edi = path + new_edi_name\n",
    "    coh_orig = path + coh\n",
    "    new_coh = path + new_coh_name\n",
    "    shutil.move(edi, new_edi)\n",
    "    shutil.move(coh_orig, new_coh)\n",
    "    # edi = glob(os.path.join(out_dir, '*.edi'))[0]\n",
    "    plotedi(new_edi, saveplot=True)\n",
    "    return\n",
    "\n",
    "def merge_birrp_results(out_dir, site_name):\n",
    "    edi_fns = (glob(os.path.join(out_dir, 'undecimated', '*edi')) +\n",
    "               glob(os.path.join(out_dir, 'decimated10', '*edi')) +\n",
    "               glob(os.path.join(out_dir, 'decimated100', '*edi')))\n",
    "    e1 = mtpy.core.edi.Edi(edi_fns[0])\n",
    "    e2 = mtpy.core.edi.Edi(edi_fns[1])\n",
    "    out_small = os.path.join(out_dir, 'small')\n",
    "    mtpy.core.edi.combine_edifiles(edi_fns[0], edi_fns[1], out_fn=out_small,\n",
    "                                   merge_freq=e1.freq[-2])\n",
    "    out_large = os.path.join(out_dir, site_name)\n",
    "    in_small = out_small + '_merged.edi'\n",
    "    mtpy.core.edi.combine_edifiles(in_small, edi_fns[2], out_fn=out_large,\n",
    "                                   merge_freq=e2.freq[-2])\n",
    "    os.remove(in_small)\n",
    "    final_fn = out_large + '_merged.edi'\n",
    "    plotedi(final_fn, saveplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each EDI generated is also plotted. This code makes use of the ``mtpy`` [(Krieger and Peacock, 2014)](https://www.sciencedirect.com/science/article/pii/S0098300414001794) library.\n",
    "\n",
    "Note that this script requires both a instrument response function file, and a survey configuration filename. Details on the survey configuration file can be found in the ``mtpy`` package, however it will looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[RM0105]\n",
    "station_type = mt\n",
    "lat = -34.05543333333333\n",
    "lon = 140.62353333333334\n",
    "elev = 0\n",
    "sampling = 0.002\n",
    "loc = Renmark\n",
    "acqdate = 2009\n",
    "acquired_by = The_University_of_Adelaide\n",
    "\n",
    "\n",
    "[RM0106]\n",
    "station_type = mt\n",
    "lat = -34.041183333333336\n",
    "lon = 140.60265\n",
    "elev = 0\n",
    "sampling = 0.002\n",
    "loc = Renmark\n",
    "acqdate = 2009\n",
    "acquired_by = The_University_of_Adelaide\n",
    "\n",
    "#[RM0107]\n",
    "#station_type = mt\n",
    "#lat = -34.004266666666666\n",
    "#lon = 140.5869\n",
    "#elev = 0\n",
    "#sampling = 0.002\n",
    "#loc = Renmark\n",
    "#acqdate = 2009\n",
    "#acquired_by = The_University_of_Adelaide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And so on, for each site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "\n",
    "These functions can be wrapped into one function to process a single site with a single remote site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_site(local_name, remote_name, tmp_dir, **kwargs):\n",
    "    dirs = {'undecimated': 0.002, 'decimated10': 0.02, 'decimated100': 0.2}\n",
    "    sites = get_metadata(kwargs['base_dir'], kwargs['frequency'], tmp_dir)\n",
    "    print('got metadata')\n",
    "    local_site = sites[local_name]\n",
    "    remote_site = sites[remote_name]\n",
    "    dir_name = local_site['name'] + remote_site['name']\n",
    "    out_dir = os.path.join(tmp_dir, dir_name)\n",
    "    try:\n",
    "        os.makedirs(out_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    cwd = os.getcwd()\n",
    "    samples, tstart = write_birrp_inputs(local_site, remote_site, out_dir)\n",
    "    print('written inputs')\n",
    "    for dir_name in dirs.keys():\n",
    "        sample_rate = dirs[dir_name]\n",
    "        dir_name = os.path.join(out_dir, dir_name)\n",
    "        os.chdir(dir_name)\n",
    "        birrp_script = gen_birrp_script(dir_name, samples, sample_rate,\n",
    "                                        local_name, tstart)\n",
    "        run_birrp(dir_name, birrp_script, birrp_location)\n",
    "        print('ran birrp')\n",
    "        process_birrp_results(dir_name, kwargs['survey_cfg_fn'],\n",
    "                              local_site['name'], kwargs['instr_resp_fn'])\n",
    "        print('processed results')\n",
    "        os.chdir(cwd)\n",
    "    merge_birrp_results(out_dir, local_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we save all of this to a file as ``nci_birrp.py`` we can create a separate file in the same folder which calls it and runs one site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python2.7\n",
    "\n",
    "import sys\n",
    "from nci_birrp import *\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kwargs = {}\n",
    "    base_dir = '/g/data/my80/States_and_Territories/SA/Broadband/Renmark_2009/TS'\n",
    "    birrp_location = '/g/data/up99/sandbox/bulk_processing_test/birrp/birrp' ### This version of BIRRP was compiled specifically for the Renmark 2009 use case\n",
    "    survey_cfg_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/survey_config/renmark.cfg'\n",
    "    instr_resp_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/instrument_response_function/lemi_coils_instrument_response_freq_real_imag.txt'\n",
    "    kwargs['instr_resp_fn'] = instr_resp_fn\n",
    "    kwargs['survey_cfg_fn'] = survey_cfg_fn\n",
    "    kwargs['birrp_location'] = birrp_location\n",
    "    kwargs['base_dir'] = base_dir\n",
    "    kwargs['frequency'] = 500\n",
    "    run_one_site(sys.argv[1], sys.argv[2], sys.argv[3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this as ``proc_site.py``.\n",
    "\n",
    "Here we enter the parameters specific to our example. For our example we use the Renmark 2009 MT dataset from the NCI, which is recorded at 500 Hz and has a configuration file located at `/g/data/up99/sandbox/bulk_birrp_processing_test/survey_config/renmark.cfg`.\n",
    "\n",
    "The system arguments are site specific, with the local site name, remote site name, and temporary working directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on the NCI\n",
    "\n",
    "We are now almost ready to process the entire survey on the NCI. First we will need a script to submit ``proc_site.py`` to process the data from one site with a single remote site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will submit the job to the NCI. Take care to choose optimal values for the walltime and memory consumption, as these will change depending on the recording length. Our example PBS script is using project up99 (change this to a compute project code that you are a member of), the normal queue, a walltime of 3 hours, 24 CPUs and 64 GB of memory. Please see [PBS directives explained](https://opus.nci.org.au/display/Help/PBS+Directives+Explained) for more information on the PBS flags.  \n",
    "\n",
    "We can now loop over each combination of local and remote sites, check to see if there is sufficient overlap, and if so, submit a job to the NCI to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def add_shell_run(local_site, remote_site, base_dir, tmp_dir):\n",
    "    python_call = proc_site.format(local_site, remote_site, tmp_dir)\n",
    "    shell_script = '\\n'.join(['#!/bin/bash', '', '#PBS -P up99', '#PBS -q normal',\n",
    "                            '#PBS -l walltime=3:00:00,mem=64GB,ncpus=24,jobfs=30GB','#PBS -l storage=gdata/my80+gdata/up99',\n",
    "                            '#PBS -l wd', '', 'module load python2/2.7.17',\n",
    "                            'module load intel-compiler/2020.2.254',\n",
    "                            'module use /g/data/up99/modulefiles/',\n",
    "                            'module load mtpy_nci_processing/2013_0.0.1','module load openmpi/4.0.2', '', python_call])\n",
    "    shell_fn = os.path.join(tmp_dir, local_site + '_' + remote_site + '.sh')\n",
    "    with open(shell_fn, 'w') as f:\n",
    "        f.write(shell_script)\n",
    "    os.system('qsub {}'.format(shell_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mtpy.utils.configfile\n",
    "\n",
    "def loop_sites(base_dir, tmp_dir, survey_cfg_fn=None):\n",
    "    sites = get_metadata(base_dir, 500, tmp_dir)\n",
    "    if survey_cfg_fn:\n",
    "        survey_cfg = mtpy.utils.configfile.read_survey_configfile(survey_cfg_fn)\n",
    "    site_names = sites.keys()\n",
    "    for site in site_names:\n",
    "       if site not in survey_cfg:\n",
    "            sites.pop(site, None)\n",
    "    print(sites)\n",
    "    for local_site in sites:\n",
    "        for remote_site in sites:\n",
    "            if calc_intersection(sites[local_site], sites[remote_site]):\n",
    "                print('add {} and {}'.format(local_site, remote_site))\n",
    "                add_shell_run(local_site, remote_site, base_dir, tmp_dir)\n",
    "                #sys.exit()  ####uncomment this line if you are troubleshooting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An option is also included to pass a survey configuration file (as detailed above), where you can comment out any sites which you do not want to process. \n",
    "\n",
    "Let's write our ``nci_birrp.py`` and ``proc_site.py`` files to our local working directory on Gadi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing nci_birrp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile nci_birrp.py\n",
    "\n",
    "# module load birrp\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import mtpy.core.edi\n",
    "from mtpy.uofa.qel_monitoring_j2edi import convert2edi\n",
    "from mtpy.uofa.simpleplotEDI import plotedi\n",
    "import mtpy.utils.configfile\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "#######################################################################\n",
    "#                                                                     #\n",
    "# Note that you will have to change the following variables to match  #\n",
    "# the time series data location, birrp executable location, survey    #\n",
    "# config file, instrument response function, temporary directory,     #\n",
    "# recorded frequency and and proc_site.py file you are using          #\n",
    "#                                                                     #\n",
    "#######################################################################\n",
    "\n",
    "base_dir = '/g/data/my80/States_and_Territories/SA/Broadband/Renmark_2009/TS'\n",
    "birrp_location = '/g/data/up99/sandbox/bulk_birrp_processing_test/birrp/birrp' ### BIRRP compiled for this Renmark 2009 use case\n",
    "survey_cfg_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/survey_config/renmark.cfg'\n",
    "instr_resp_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/instrument_response_function/lemi_coils_instrument_response_freq_real_imag.txt'\n",
    "tmp_dir = '/g/data/up99/sandbox/bulk_birrp_processing_test/processing_directory/'\n",
    "frequency = 500\n",
    "proc_site = '/g/data/up99/sandbox/bulk_birrp_processing_test/processing_directory/proc_site.py {} {} {}'\n",
    "\n",
    "def _linecount(filename):\n",
    "    \"\"\" Return number of lines in a file \"\"\"\n",
    "    num_lines = sum(1 for line in open(filename))\n",
    "    return num_lines\n",
    "\n",
    "\n",
    "def get_metadata(base_dir, frequency, tmp_dir):\n",
    "    \"\"\" Return information about each site \"\"\"\n",
    "    fn = os.path.join(tmp_dir, 'metadata.json')\n",
    "    if os.path.exists(fn):\n",
    "        with open(fn) as f:\n",
    "            sites = pickle.load(f)\n",
    "        return sites\n",
    "    sites = [os.path.basename(i) for i in glob(os.path.join(base_dir, '*'))]\n",
    "\n",
    "    sites = dict([(i, {}) for i in sites])\n",
    "\n",
    "    for site in sites.keys():\n",
    "        sites[site]['name'] = site\n",
    "        sites[site]['files'] = []\n",
    "        days = sorted([os.path.basename(i) for\n",
    "                       i in glob(os.path.join(base_dir, site, '*'))])\n",
    "        for idx, day in enumerate(days):\n",
    "            files = glob(os.path.join(base_dir, site, day, '*'))\n",
    "            if not files:\n",
    "                continue\n",
    "            last_files = files\n",
    "            sites[site]['files'].append(files)\n",
    "            if idx == 0:\n",
    "                start_time = os.path.basename(files[0]).split('_')[1].split('.')[0]\n",
    "                start_time = datetime.datetime.strptime(start_time,\n",
    "                                                        '%y%m%d%H%M%S')\n",
    "                sites[site]['start_time'] = start_time\n",
    "        if not [j for k in sites[site]['files'] for j in k]:\n",
    "            del sites[site]\n",
    "            continue\n",
    "        sites[site]['files'] = [j for k in sites[site]['files'] for j in k]\n",
    "        print(site, sites[site]['files'])\n",
    "        #if not files:\n",
    "        #    continue\n",
    "        files = last_files\n",
    "        end_date = os.path.basename(files[0]).split('_')[1].split('.')[0]\n",
    "        end_date = datetime.datetime.strptime(end_date, '%y%m%d%H%M%S')\n",
    "        length = _linecount(files[0])\n",
    "        end_time = end_date + datetime.timedelta(seconds=length/frequency)\n",
    "        sites[site]['end_time'] = end_time\n",
    "        sites[site]['samples'] = (sites[site]['end_time'] -\n",
    "                                  sites[site]['start_time']).seconds * frequency\n",
    "\n",
    "    with open(fn, 'w') as f:\n",
    "        pickle.dump(sites, f)\n",
    "    return sites\n",
    "\n",
    "\n",
    "def calc_intersection(local_site, remote_site):\n",
    "    \"\"\" Calculate time overlap of a local site and remote site \"\"\"\n",
    "    int_start = max(local_site['start_time'], remote_site['start_time'])\n",
    "    int_end = min(local_site['end_time'], remote_site['end_time'])\n",
    "    if (int_end - int_start).total_seconds()/60/60 < 5:\n",
    "        return\n",
    "    else:\n",
    "        return int_start, int_end\n",
    "\n",
    "\n",
    "def decimate_file(fn, decimation, new_fn):\n",
    "    \"\"\" Read in a time series, apply a decimation and write out \"\"\"\n",
    "    print(fn, decimation, new_fn)\n",
    "    print(os.path.exists(new_fn))\n",
    "    if os.path.exists(new_fn):\n",
    "         return\n",
    "    print('decimating {}'.format(fn))\n",
    "    f = np.genfromtxt(fn)\n",
    "    f = scipy.signal.decimate(f, decimation, n=8)\n",
    "    np.savetxt(new_fn, f)\n",
    "\n",
    "\n",
    "def write_files(files, num_skip, num_samples, channel, out_dir, tstart, remote=False):\n",
    "    \"\"\" Write files \"\"\"\n",
    "    fn = 'local.' + channel if not remote else 'remote.' + channel\n",
    "    fn = tstart.strftime('%y%m%d%H%M%S') + '_' + fn\n",
    "    if os.path.exists(os.path.join(out_dir, fn)):\n",
    "        return\n",
    "    print('writing to {}'.format(os.path.join(out_dir, fn)))\n",
    "    ofile = open(os.path.join(out_dir, fn), 'w')\n",
    "    ifile = open(files.pop(0))\n",
    "    print(files, num_skip, num_samples)\n",
    "    for _ in range(num_skip):\n",
    "        try:\n",
    "            next(ifile)\n",
    "        except StopIteration:\n",
    "            ifile = open(files.pop(0))\n",
    "    for _ in range(num_samples):\n",
    "        try:\n",
    "            line = next(ifile)\n",
    "        except StopIteration:\n",
    "            try:\n",
    "                ifile = open(files.pop(0))\n",
    "                line = next(ifile)\n",
    "            except IndexError:\n",
    "                pass\n",
    "                # print('did not extract from {}'.format(files))\n",
    "        ofile.write(line)\n",
    "\n",
    "\n",
    "        \n",
    "def write_birrp_inputs(local_site, remote_site, out_dir):\n",
    "    \"\"\" Write out the intersection of two files \"\"\"\n",
    "    if calc_intersection(local_site, remote_site):\n",
    "        int_start, int_end = calc_intersection(local_site, remote_site)\n",
    "    local_skip = (int_start - local_site['start_time']).total_seconds()*500\n",
    "    remote_skip = (int_start - remote_site['start_time']).total_seconds()*500\n",
    "    local_skip = int(local_skip)\n",
    "    remote_skip = int(remote_skip)\n",
    "    num_samples = int((int_end - int_start).total_seconds()*500)\n",
    "    if local_site['name'] == remote_site['name']:\n",
    "        remote_skip += 1\n",
    "        num_samples -= 1\n",
    "    dec_dir = os.path.join(out_dir, 'undecimated')\n",
    "    dec10_dir = os.path.join(out_dir, 'decimated10')\n",
    "    dec100_dir = os.path.join(out_dir, 'decimated100')\n",
    "    try:\n",
    "        os.makedirs(dec_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(dec10_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(dec100_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    channels = ['BX', 'BY', 'EX', 'EY']\n",
    "    for channel in channels:\n",
    "        files = sorted([i for i in local_site['files'] if channel in i])\n",
    "        write_files(files, local_skip, num_samples, channel, dec_dir, int_start)\n",
    "    for channel in [i for i in channels if 'B' in i]:\n",
    "        files = sorted([i for i in remote_site['files'] if channel in i])\n",
    "        write_files(files, remote_skip, num_samples, channel, dec_dir, int_start,\n",
    "                    remote=True)\n",
    "    for fn in glob(os.path.join(dec_dir, '*')):\n",
    "        if fn.endswith('X') or fn.endswith('Y'):\n",
    "            bn = os.path.basename(fn)\n",
    "            new_fn = os.path.join(dec10_dir, bn)\n",
    "            decimate_file(fn, 10, new_fn)\n",
    "            fn = new_fn\n",
    "            new_fn = os.path.join(dec100_dir, bn)\n",
    "            decimate_file(fn, 10, new_fn)\n",
    "    return num_samples, int_start\n",
    "\n",
    "\n",
    "def gen_birrp_script(out_dir, samples, sample_rate, site_name, tstart):\n",
    "    birrp_string = '\\n'.join(['1', '2', '2', '2', '1', '3', '{2}',\n",
    "                              '65536,2,13', '3,1,3', 'y', '2',\n",
    "                              '0,0.0003,0.9999', '0.7', '0.0', '0.003,10000', '{3}', '0', '0',   \n",
    "                              '1', '10', '0', '0', '{1}', '0', '{4}_local.EY',\n",
    "                              '0', '0', '{4}_local.EX', '0', '0', '{4}_local.BY',\n",
    "                              '0', '0', '{4}_local.BX', '0', '0', '{4}_remote.BY',\n",
    "                              '0', '0', '{4}_remote.BX', '0', '0,90,0', '0,90,0',\n",
    "                              '0,90,0'])\n",
    "    birrp_string = birrp_string.format(os.path.join(out_dir, ''),\n",
    "                                       samples, sample_rate, site_name,\n",
    "                                       tstart.strftime('%y%m%d%H%M%S'))\n",
    "    return birrp_string\n",
    "\n",
    "def run_birrp(out_dir, birrp_script, birrp_location):\n",
    "    script_fn = os.path.join(out_dir, 'script.txt')\n",
    "    with open(script_fn, 'w') as f:\n",
    "        f.write(birrp_script)\n",
    "    if any(fn.endswith('.j') for fn in os.listdir(out_dir)):\n",
    "        return\n",
    "    #os.system('mpirun -np 32 {} < {}'.format(birrp_location, script_fn)) ### Uncomment if using the mpi modified version of BIRRP\n",
    "    os.system('{} < {}'.format(birrp_location, script_fn))\n",
    "    for f in glob('fft.*'):\n",
    "        os.remove(f)\n",
    "    #for f in glob('remote.*') + glob('local.*'):  ###uncomment this for loop if you want to remove the intermediate files\n",
    "    #    #os.remove(f)\n",
    "\n",
    "    \n",
    "def process_birrp_results(out_dir, survey_cfg_fn, site_name, instr_resp_fn):\n",
    "    edi, coh = convert2edi(site_name, out_dir, survey_cfg_fn, instr_resp_fn, None)\n",
    "    metadata =  mtpy.utils.configfile.read_survey_configfile(survey_cfg_fn)[site_name.upper()]\n",
    "    e = mtpy.core.edi.Edi(edi)\n",
    "    if 'loc' in metadata:\n",
    "        e.head['loc'] = metadata['loc']\n",
    "    else:\n",
    "        e.head['loc'] = None\n",
    "    if 'acqdate' in metadata:\n",
    "        e.head['acqdate'] = metadata['acqdate']\n",
    "    else:\n",
    "        e.head['acqdate'] = None\n",
    "    if 'acquired_by' in metadata:\n",
    "        e.head['acqby'] = metadata['acquired_by']\n",
    "    else:\n",
    "        e.head['acqby'] = ''\n",
    "    edi = e.writefile(edi, allow_overwrite=True)\n",
    "    path = edi.split(\"qel\")[0]\n",
    "    edi_file = os.path.basename(edi)\n",
    "    new_name = edi_file.split(\"_\")[1]\n",
    "    new_edi_name = new_name + '.edi'\n",
    "    new_coh_name = new_name + '.coh'\n",
    "    new_edi = path + new_edi_name\n",
    "    coh_orig = path + coh\n",
    "    new_coh = path + new_coh_name\n",
    "    shutil.move(edi, new_edi)\n",
    "    shutil.move(coh_orig, new_coh)\n",
    "    # edi = glob(os.path.join(out_dir, '*.edi'))[0]\n",
    "    plotedi(new_edi, saveplot=True)\n",
    "    return\n",
    "\n",
    "def merge_birrp_results(out_dir, site_name):\n",
    "    edi_fns = (glob(os.path.join(out_dir, 'undecimated', '*edi')) +\n",
    "               glob(os.path.join(out_dir, 'decimated10', '*edi')) +\n",
    "               glob(os.path.join(out_dir, 'decimated100', '*edi')))\n",
    "    e1 = mtpy.core.edi.Edi(edi_fns[0])\n",
    "    e2 = mtpy.core.edi.Edi(edi_fns[1])\n",
    "    out_small = os.path.join(out_dir, 'small')\n",
    "    mtpy.core.edi.combine_edifiles(edi_fns[0], edi_fns[1], out_fn=out_small,\n",
    "                                   merge_freq=e1.freq[-2])\n",
    "    out_large = os.path.join(out_dir, site_name)\n",
    "    in_small = out_small + '_merged.edi'\n",
    "    mtpy.core.edi.combine_edifiles(in_small, edi_fns[2], out_fn=out_large,\n",
    "                                   merge_freq=e2.freq[-2])\n",
    "    os.remove(in_small)\n",
    "    final_fn = out_large + '_merged.edi'\n",
    "    plotedi(final_fn, saveplot=True)\n",
    "\n",
    "    \n",
    "def run_one_site(local_name, remote_name, tmp_dir, **kwargs):\n",
    "    dirs = {'undecimated': 0.002, 'decimated10': 0.02, 'decimated100': 0.2}\n",
    "    sites = get_metadata(kwargs['base_dir'], kwargs['frequency'], tmp_dir)\n",
    "    print('got metadata')\n",
    "    local_site = sites[local_name]\n",
    "    remote_site = sites[remote_name]\n",
    "    dir_name = local_site['name'] + remote_site['name']\n",
    "    out_dir = os.path.join(tmp_dir, dir_name)\n",
    "    try:\n",
    "        os.makedirs(out_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    cwd = os.getcwd()\n",
    "    samples, tstart = write_birrp_inputs(local_site, remote_site, out_dir)\n",
    "    print('written inputs')\n",
    "    for dir_name in dirs.keys():\n",
    "        sample_rate = dirs[dir_name]\n",
    "        dir_name = os.path.join(out_dir, dir_name)\n",
    "        os.chdir(dir_name)\n",
    "        birrp_script = gen_birrp_script(dir_name, samples, sample_rate,\n",
    "                                        local_name, tstart)\n",
    "        run_birrp(dir_name, birrp_script, birrp_location)\n",
    "        print('ran birrp')\n",
    "        process_birrp_results(dir_name, kwargs['survey_cfg_fn'],\n",
    "                              local_site['name'], kwargs['instr_resp_fn'])\n",
    "        print('processed results')\n",
    "        os.chdir(cwd)\n",
    "    merge_birrp_results(out_dir, local_name)\n",
    "\n",
    "def add_shell_run(local_site, remote_site, base_dir, tmp_dir):\n",
    "    python_call = proc_site.format(local_site, remote_site, tmp_dir)\n",
    "    shell_script = '\\n'.join(['#!/bin/bash', '', '#PBS -P up99', '#PBS -q normal',\n",
    "                            '#PBS -l walltime=3:00:00,mem=64GB,ncpus=24,jobfs=30GB','#PBS -l storage=gdata/my80+gdata/up99',\n",
    "                            '#PBS -l wd', '', 'module load python2/2.7.17',\n",
    "                            'module load intel-compiler/2020.2.254',\n",
    "                            'module use /g/data/up99/modulefiles/',\n",
    "                            'module load mtpy_nci_processing/2013_0.0.1','module load openmpi/4.0.2', '', python_call])\n",
    "    shell_fn = os.path.join(tmp_dir, local_site + '_' + remote_site + '.sh')\n",
    "    with open(shell_fn, 'w') as f:\n",
    "        f.write(shell_script)\n",
    "    os.system('qsub {}'.format(shell_fn))\n",
    "\n",
    "\n",
    "def loop_sites(base_dir, tmp_dir, survey_cfg_fn=None):\n",
    "    sites = get_metadata(base_dir, 500, tmp_dir)\n",
    "    if survey_cfg_fn:\n",
    "        survey_cfg = mtpy.utils.configfile.read_survey_configfile(survey_cfg_fn)\n",
    "    site_names = sites.keys()\n",
    "    for site in site_names:\n",
    "       if site not in survey_cfg:\n",
    "            sites.pop(site, None)\n",
    "    print(sites)\n",
    "    for local_site in sites:\n",
    "        for remote_site in sites:\n",
    "            if calc_intersection(sites[local_site], sites[remote_site]):\n",
    "                print('add {} and {}'.format(local_site, remote_site))\n",
    "                add_shell_run(local_site, remote_site, base_dir, tmp_dir)\n",
    "                #sys.exit()  ####uncomment line if you are troubleshooting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing proc_site.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile proc_site.py\n",
    "\n",
    "#!/usr/bin/env python2.7\n",
    "\n",
    "import sys\n",
    "from nci_birrp import *\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    kwargs = {}\n",
    "    base_dir = '/g/data/my80/States_and_Territories/SA/Broadband/Renmark_2009/TS'\n",
    "    birrp_location = '/g/data/up99/sandbox/bulk_birrp_processing_test/birrp/birrp'\n",
    "    survey_cfg_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/survey_config/renmark.cfg'\n",
    "    instr_resp_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/instrument_response_function/lemi_coils_instrument_response_freq_real_imag.txt'\n",
    "    kwargs['instr_resp_fn'] = instr_resp_fn\n",
    "    kwargs['survey_cfg_fn'] = survey_cfg_fn\n",
    "    kwargs['birrp_location'] = birrp_location\n",
    "    kwargs['base_dir'] = base_dir\n",
    "    kwargs['frequency'] = 500\n",
    "    run_one_site(sys.argv[1], sys.argv[2], sys.argv[3], **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for our example, we are using a BIRRP version compiled specifically for the Renmark 2009 use case (located in ``/g/data/up99/sandbox/bulk_birrp_processing_test/birrp/birrp``). There is also an MPI parallelised version of BIRRP that can be used (located in ``/g/data/up99/sandbox/bulk_birrp_processing_test/birrp_mpi``) - this version runs much faster on the undecimated data, but was not compiled to work with the decimated100 dataset. \n",
    "\n",
    "Now let's log onto Gadi and change into our working directory where the ``nci_birrp.py`` and ``proc_site.py`` files are located. Next we need to run the following commands to set up our local environment: \n",
    "\n",
    "    $ module purge\n",
    "    \n",
    "    $ module load pbs\n",
    "    \n",
    "    $ module load python2/2.7.17\n",
    "    \n",
    "    $ module use /g/data/up99/modulefiles\n",
    "    \n",
    "    $ module load mtpy_nci_processing/2013_0.0.1\n",
    "    \n",
    "    $ module load openmpi/4.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To process the entire Renmark MT survey we would need, for example, to open up an IPython terminal and execute the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run nci_birrp.py\n",
    "\n",
    "base_dir = '/g/data/my80/States_and_Territories/SA/Broadband/Renmark_2009/TS/'\n",
    "birrp_location = '/g/data/up99/sandbox/bulk_birrp_processing_test/birrp/birrp'\n",
    "survey_cfg_fn = '/g/data/up99/sandbox/bulk_birrp_processing_test/survey_config/renmark.cfg'\n",
    "loop_sites(base_dir, tmp_dir, survey_cfg_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the progress of the jobs in their queue by running ``qstat -x`` in a bash terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "qstat -x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the jobs have executed on the NCI, the processed EDIs and plots would be found in your tmp_dir, which for this example was ``/g/data/up99/sandbox/bulk_birrp_processing_test/processing_directory/``.\n",
    "\n",
    "If the processing fails, the cause can be debugged using the error file created when running the job. These are located in the same directory as the jobs are submitted (in our case the directory in which this script is run)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
